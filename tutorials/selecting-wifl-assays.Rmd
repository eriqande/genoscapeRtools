---
title: "Selecting WIFL Assays"
output:
  html_notebook:
    toc: yes
  github_document:
    toc: yes
date: '`r format(Sys.time(), "%d %B, %Y")`'
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "selecting-wifl-assays-figs/"
)
```


## Load some packages

```{r libs}
library(readr)
library(dplyr)
library(ggplot2)
library(stringr)
library(genoscapeRtools)
```


## Introduction

This is a bit of a mess, but we have something of a way forward, having looked at many of the
different comparisons.  We are going to look at every relevant between-subspecies comparison
and every relevant within-subspecies comparison, and just imagine taking the top 300 from 
each.  Then we will filter by assayability and see how many we end up with.

I have wrapped a lot of the gritty work into a few functions.

### Get the SNPs 
We are going to develop SNPs from amongst those that passed our "clean" filters:
```{r read-snps, message=FALSE}
wifl_clean <- read_012("../cleaned_indv175_pos105000")
```


### Get and process the groups
We also want the groups that we are putting the birds into:
Here is the file of designations that Kristen has 
supplied:
```{r group-desig, message=FALSE}
grps <- read_delim("../development/data/WIFL_Cat_forAssayDesign.txt", delim = "\t")
grps
```

That is pretty straightforward.  Let's just count up how many birds we have in each group and subgroup:
```{r count-grps, rows.print = 30}
grps %>%
  group_by(Subspecies_Pure, Within_Subspecies) %>%
  tally()
```
This is good to have a look at.  It makes it clear that these categories are not strictly nested.  But we have plenty of 
birds to choose from, so that is good.

We need to get a list of the IDs of the individuals in each of those groups:
```{r get-lists}
major_grps <- grps %>%
  filter(!is.na(Subspecies_Pure)) %>%
  split(.$Subspecies_Pure) %>%
  lapply(., function(x) x$Individual)

minor_grps <- grps %>%
  filter(!is.na(Within_Subspecies)) %>%
  split(.$Within_Subspecies) %>%
  lapply(., function(x) x$Individual)

# see what that looks like
minor_grps[1:2]
```

### Computing allele frequencies in the groups

Now we can do that with a function from genoscapeRtools:
```{r get-freqs}
major_grp_freqs <- group_012_cnts(wifl_clean, major_grps)
minor_grp_freqs <- group_012_cnts(wifl_clean, minor_grps)
```

## Doing the comparisons

Here we have some messy stuff to just pick out the comparisons that
we want to look at, and then we actually make a big data frame of all
those comparisons.

### The "major" comparisons (between subspecies level groups)

```{r prep-comps}
comps <- combn(unique(major_grp_freqs$group), 2)
keep <- str_detect(comps, "ext") %>%
  matrix(nrow = 2) %>%
  colSums() < 2
comps <- comps[, keep]

major_comps <- lapply(1:ncol(comps), function(c) {
  comp_pair(major_grp_freqs, comps[1, c], comps[2, c])
}) %>%
  bind_rows()

# print out how many that is:
major_comps %>% group_by(comparison) %>% tally()

```

### The "minor" comparison (within subspecies)

we will only look within subspecies in these cases
```{r prep-comps2}
comps2 <- combn(unique(minor_grp_freqs$group), 2)

# now restrict that to just the comparisons within subspp
pref <- str_replace_all(comps2, "_.*", "") %>%
  matrix(nrow = 2)
keep2 <- pref[1,] == pref[2,]

comps2 <- comps2[, keep2]

minor_comps <- lapply(1:ncol(comps2), function(c) {
  comp_pair(minor_grp_freqs, comps2[1, c], comps2[2, c])
}) %>%
  bind_rows()

# print out how many that is:
minor_comps %>% group_by(comparison) %>% tally()
```

## Choosing the top 300 for each comparison
This is relatively easy, it is just filtering.  We rank things and then take the top 300.  This might
be slightly less than 300 if there are ties, but that is OK, I think.  We will stick them 
together at the end.
```{r rank-em}
top300 <- lapply(list(major = major_comps, minor = minor_comps), function(x) {
  x %>%
    group_by(comparison) %>%
    mutate(rank_full = rank(-cap)) %>%
    filter(rank_full <= 300)
}) %>%
  bind_rows(.id = "type_of_comparison")

# we print that out here so it can be seen
top300
```
There are `r nrow(top300)` rows in that data frame, but the total number of unique SNPs is smaller: `r length(unique(top300$pos))`
Methinks that would be a wonderful set to use if we were able to economically use baits and NGS to get the 
genotypes.  That might be possible some day.

## Whittling these down by assayability

For this, we are going to take `top300` and just filter it by whether or not it is assayable.  We define it as assayble if it doesn't
have any SNP with MAF > 0.02 (and taken from our less filtered data set where a SNP is called if it appears in at least 50% of the 
individuals) with 20 bp.

First, get that "less filtered" data. 
This is from the  012 file with 350K SNPs:
```{r readit, message=FALSE}
wifl <- read_012(prefix = "~/Documents/UnsyncedData/WIFL_10-15-16/wifl-10-15-16", gz = TRUE)
```

And then we assess the assayability of these all. The first thing we are going to do
is just filter on MAF > 0.02 to get all the SNPs (all the variation, really, since
we are going to ignore the chance of indels) that we might want to worry about as 
far as whether they are in the flanking regions.  While we are at this, we are
going to write all those positions to a file so we can pull them out of the 
VCF file later so that we can get what the REF and ALT alleles are.  
```{r snps-to-care}
# keep track of which of the SNPs we are going to "worry about"
# as far as whether they might be in the flanking region
snps_to_care_about <- compile_assayable(wifl, minMAF = 0.02, flank = 20)

# while we are at it.  Let's write out a tab delimited file of these positions
# so we can easily pull them out of the VCF file later 
snps_to_care_about %>%
  select(snp) %>%
  tidyr::separate(snp, into = c("CHROM", "POS"), sep = "--") %>%
  write.table(., quote = FALSE, row.names = FALSE, col.names = FALSE, file = "snps-to-care-about.txt", sep = "\t")


```

Then keep track of which ones are assayable.
```{r get-assayable}
cassay <- snps_to_care_about %>%
  filter(assayable == TRUE)  # we will keep only the assayable ones (possibly on edges)
cassay
```

So, now we can filter `top300` by whether they are assayable or not, and then 
join them:
```{r join-assayable, message=FALSE}
top_assayable <- top300 %>%
  rename(snp = pos) %>%
  filter(snp %in% cassay$snp) %>%
  left_join(., cassay)
```
That gives us `r length(unique(top_assayable$snp))` unique SNPs to play around with.  If we wanted to filter it even harder, we could
throw out ones that were on the edges (i.e. no SNP within 1000 bp of it).  
```{r top-edgeless}
top_off_edge <- top_assayable %>%
  filter(on_left_edge == FALSE, on_right_edge == FALSE)
```
which gives us `r length(unique(top_off_edge$snp))` unique SNPs.

To give us a sense for what sorts of things we are looking at here, let's look at the 40 best from each 
of the comparisons.  And we want to record how many of the top 40 each SNP is in
```{r looksie, rows.print=40, message=FALSE}

tmp <- top_off_edge %>%
  select(type_of_comparison, snp, leftpad, rightpad, comparison, freq.x, freq.y, ntot.x, ntot.y, cap, rank_full) %>%
  group_by(type_of_comparison, comparison) %>%
  top_n(40, -rank_full) %>%
  mutate(idx = 1:n()) %>%  # this and the next two lines are just so that we get exactly
  filter(idx <= 40) %>%    # 20---the ties usually occur when the sample size is small and so
  select(-idx)  %>%           # it is OK to pitch them.
  ungroup()

num_occur <- tmp %>%
  group_by(snp) %>%
  summarise(num_comps = n())

top40 <- left_join(tmp, num_occur)
  
```
Before we print that out, I want to join the LFMM scores onto it:
```{r get-lfmm, message=FALSE, rows.print=40}
lfmm <- readr::read_delim("~/Documents/UnsyncedData/WIFL/WIFL_LFMMresults.txt", delim = "\t") %>%
  tidyr::gather(key = "variable", value = "fdr_p", -scaffold, -pos) %>%
  tidyr::unite(snp, scaffold, pos, sep = "--")

# and now filter it to the smallest p-value for each snp:
lfmm_mins <- lfmm %>%
  group_by(snp) %>%
  filter(fdr_p == min(fdr_p))

# and now we join that on there
top40_lfmm <- top40 %>%
  left_join(., lfmm_mins)

# then print it
top40_lfmm
```
It is possible to flip through that and pretty quickly see what is going on.  There are `r length(unique(top40$snp))` unique SNPs there, and I think we can just 
look through them and pretty easily whittle that down to 192, or as many as we think we might want to develop into assays.

Just for fun, let's look at the distribution of all the min LFMM p-values in these SNPs compared to all the rest:
```{r plot-lfmm-mins}
# first get a data frame of just one occurrence per SNP
one_each <- top40_lfmm %>%
  group_by(snp) %>%
  mutate(tmp = 1:n()) %>%
  filter(tmp == 1)

# then plot
ggplot() +
  geom_density(data = lfmm_mins, mapping = aes(x = fdr_p), alpha = "0.5", fill = "blue") +
  geom_density(data = one_each, mapping = aes(x = fdr_p), alpha = "0.5", fill = "orange")
```

Orange are the selected SNPs and blue are all SNPs.  We see that there is a differnet in the distributions
but that might just be because we have chosen SNPs that do not have very low minor allele frequencies.


## Final Step, Choosing a set of 192 SNPs

### Revisiting the actual alleles
For the final stage, we need to go back to the original VCF so that we know what the
REF and ALT alleles are, so we can put together the assay order.  We are going to 
continue to ignore the possibility of indels, which means that we just have to 
pull the snps that we care about out of the VCF file.  We don't need all the individual
data on these, so I will just return info on one individual, NULL, that is not even in the 
file.

Doing this part on hoffman:
```
[kruegg@n2238 WIFL_10.15.16]$ pwd
/u/home/k/kruegg/nobackup-klohmuel/WIFL/WIFL_10.15.16
[kruegg@n2238 WIFL_10.15.16]$ module load vcftools
[kruegg@n2238 WIFL_10.15.16]$ module load bcftools

# filter it all down....
[kruegg@n2238 WIFL_10.15.16]$ vcftools --vcf WIFL_10.15.16.recode.vcf --out design-variants  --recode  --positions snps-to-care-about.txt --indv NULL 

VCFtools - 0.1.14
(C) Adam Auton and Anthony Marcketta 2009

Parameters as interpreted:
	--vcf WIFL_10.15.16.recode.vcf
	--out design-variants
	--positions snps-to-care-about.txt
	--recode
	--indv NULL

Keeping individuals in 'keep' list
After filtering, kept 0 out of 219 Individuals
Outputting VCF file...

[kruegg@n2238 WIFL_10.15.16]$ bgzip design-variants.recode.vcf 

```
Now I can bring that file back to my laptop and tabix it:
```
2016-12-08 07:59 /WIFL/--% pwd
/Users/eriq/Documents/UnsyncedData/WIFL
2016-12-08 07:59 /WIFL/--% scp kruegg@hoffman2.idre.ucla.edu:/u/nobackup/klohmuel/kruegg/WIFL/WIFL_10.15.16/design-variants.recode.vcf.gz  ./ 

2016-12-08 08:00 /WIFL/--% tabix design-variants.recode.vcf.gz 

```
Now, we can retrieve variants overlapping a given set of regions easily:
```
# here is what our file of regions looks like
2016-12-08 08:06 /WIFL/--% cat test_pos.txt 
scaffold66|size1465758	750000	770000
scaffold68|size1967880	500000	530000

# and here we have retrieved all the SNPs in those regions
2016-12-08 08:06 /WIFL/--% tabix design-variants.recode.vcf.gz -R test_pos.txt 
scaffold66|size1465758	760102	.	C	T	6847.19	.	.	GT:AD:DP:GQ:PL
scaffold66|size1465758	760264	.	T	G	99773.2	.	.	GT:AD:DP:GQ:PL
scaffold66|size1465758	760290	.	T	C	82551.1	.	.	GT:AD:DP:GQ:PL
scaffold66|size1465758	760311	.	T	C	104446	.	.	GT:AD:DP:GQ:PL
scaffold66|size1465758	760378	.	T	G	15369.2	.	.	GT:AD:DP:GQ:PL
scaffold66|size1465758	760604	.	C	T	4966.67	.	.	GT:AD:DP:GQ:PL
scaffold68|size1967880	519980	.	C	A	2233.52	.	.	GT:AD:DP:GQ:PL
scaffold68|size1967880	520028	.	T	A	2059.61	.	.	GT:AD:DP:GQ:PL
scaffold68|size1967880	520176	.	C	T	5527.85	.	.	GT:AD:DP:GQ:PL
scaffold68|size1967880	520326	.	C	A	41058.8	.	.	GT:AD:DP:GQ:PL
scaffold68|size1967880	520437	.	G	C	2216.86	.	.	GT:AD:DP:GQ:PL
scaffold68|size1967880	520522	.	G	A	1740.29	.	.	GT:AD:DP:GQ:PL

```

### Selecting the 192 targets

#### "Divergence" SNPs 
Kristen made up a data frame showing how many SNPs to take from the top of each 
pairwise comparison.  Some pairwise comparisons were deemed unimportant and others
had low sample sizes, so we don't take many from them, etc.  That data frame looks like:
```{r, message=FALSE}
snp_pair_wts <- read_delim("inputs/wifl_snp_comparison_counts.txt", delim = "\t")
snp_pair_wts
```


#### "Climate SNPs"
But, while we are at it, we are going to include
about 25 climate-associated SNPs, too.  Kristen gave me this:
```
Here are the climatic variables with the highest loading: 

Bio11 = mean temperature of warmest quarter
bio10 = mean temp of coldest quarter
bio5 = max temp of coldest month
bio9 = mean temp of driest month
bio17 = Precip of driest quarter

We will take the top 5 most sig loci associated with each of these variables.

```
So, from the assayable SNPs we are going to want to pull those out and include them as the first
set, and then get more non-duplicated ones.  

So, since the most significant *assayable* locus is often shared between these climate variables, 
we end up taking the 8 most signicant from each, which gives us 24 unique SNPs
that we will call the "climate" variables.  
```{r get-climate-snps}
climate_vars <- c("BIO_11", "BIO_10", "BIO_5", "BIO_9", "BIO_17")
climate_snps <- lfmm %>%
  filter(variable %in% climate_vars) %>%
  left_join(cassay %>% filter(on_left_edge == FALSE, on_right_edge == FALSE), .) %>%   # limit it to just the assayable ones THAT ARE NOT ON EDGES
  group_by(variable) %>%
  top_n(n = 8, wt = -fdr_p) %>%
  group_by(snp) %>%
  arrange(fdr_p) %>%
  mutate(comparison = paste(variable, sprintf("%.2e ", fdr_p), collapse = ", "),
         type_of_comparison = "climate") %>%
  select(-variable, -fdr_p) %>%
  filter(!duplicated(snp)) %>%
  ungroup() %>%
  select(-(num_gene_copies:rightpos), -(assayable:on_left_edge))

climate_snps
```

#### Picking out SNPs

So, we can pick out (an expanded) number of SNPs from each comparison, as given above,
and then we add the climate SNPs on top of those, and filter out any "divergence SNPs"
that are duplicates of climate SNPs (or other divergence SNPs) 
and see how many we end up with after eliminating
duplicates and things that are too close together.  
```{r pick-em}
expand_it <- 1.8  # this is the factor by which to increase the number of SNPs from each comparison

div_candidates <- top40_lfmm %>%
  left_join(snp_pair_wts) %>%
  group_by(comparison) %>%
  arrange(desc(cap)) %>%
  mutate(idx = 1:n()) %>%
  filter(idx <= (number_markers) * expand_it) %>%  # keep only a certain number from each comparison
  ungroup() 

# now, add the climate SNPs in there, filter out the duplicates, and then mark those
# that are too close to their neighbors and count up about how many we have
candidate_snps <- bind_rows(climate_snps, div_candidates) %>%
  filter(!duplicated(snp))  %>%                      # filter out the duplicates
  tidyr::separate(snp, into = c("CHROM", "POS"), sep = "--", convert = TRUE, remove = FALSE) %>%  # get CHROM and POS back
  arrange(CHROM, POS) %>%
  group_by(CHROM) %>%
  mutate(dist = POS - lag(POS, 1),
         could_be_too_close = ifelse(!is.na(dist) & dist <= 10^4, TRUE, FALSE),       # flag them if closer than 10 KB
         toss_it = FALSE)  # we add the toss_it column since we will modify that by hand to do final thinning

# now return the approximate number after thinning
sum(!candidate_snps$could_be_too_close)
```
So, that has given us, potentially, 210 candidates.  That is more than 192, but some of them 
might not be designable because of the GC content.  But, now we need to decide which 
ones to toss when they are too close together on the genome.  There are only a few of these 
and it will be good to get eyes on the data anyway. So, at this point we write out the 
file and then Kristen chooses which ones to toss by putting a "TRUE" in the "toss_it" 
column and saving the result in exactly the same format in a file named 
`candidate_snps_tossed.csv`.

```{r write-candidates}
write_csv(candidate_snps, path = "candidate_snps_untossed.csv")
```
